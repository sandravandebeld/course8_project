---
title: "Course 8 project"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
The goal of your project is to predict the manner in which people did the exercise. the data is provided by: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

### data load and preprocessing
below I load the data. I see that there are a lot of NA's within variables.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(readr)
library(caret)
pml_training <- read_csv("pml-training.csv")
pml_testing <- read_csv("pml-testing.csv")
NA_list <- colSums(is.na(pml_training))/nrow(pml_training) > 0.5
pml_training1 <- pml_training[, !NA_list]
```


### data load and preprocessing
there are still 60 variables which is a lot. To see if this could be reduced I perform a test for zero covariates. 

```{r cars, echo=FALSE, warning=FALSE, message=FALSE}
nearzero <- nearZeroVar(pml_training1, saveMetrics=TRUE)
summary(nearzero)
nearzero1 <- nearzero$nzv
df_train <- pml_training1[, !nearzero1]

```

### apply on test set
these changes are aso performed with the test dataset

```{r pressure, echo=FALSE}
pml_testing1 <- pml_testing[, !NA_list]
df_test <- pml_testing1[, !nearzero1]
```

## create training and validation set
the dataset is seperated into 2 dataset, the training dataset is used to train different models. 
The validation dataset is used to estimate how accurate the different models are for out sample errors. 
```{r}
set.seed(22)
inTrainIndex <- createDataPartition(df_train$classe, p=0.75)[[1]]
df_training <- df_train[inTrainIndex,]
df_validation <- df_train[-inTrainIndex,]
```

## exploration
before perfroming Machine learning models, I do some explorative analysis to get to know the data
```{r}
library(ggplot2)
df_training$classe <- as.factor(df_training$classe)
plot(df_training$classe)

```
most of the datapoints are assigned to classe A. but it seems no problem for further analysis because there are no rare events. 

## train machine learning models
to start i perform a " normal " decision tree. 
decision tree
```{r}
decisiontree <- train(classe ~., data= df_training, method="rpart", trControl=trainControl("cv", number=10), tuneLength=10)
decisiontree
dt_predict <- predict(decisiontree, newdata = df_validation)
df_validation$classe <- as.factor(df_validation$classe)
confusionMatrix(df_validation$classe, dt_predict)
```

The accurancy of the final model on the training set is 99%
the accurancy is really high, be aware of overfitting. 
the acuurancy of the final mode on the validation set is 99%


## model selection
because the accurancy of the mdel is high for out of sample, it seems to be a good prediction model. therefor i did not perform any other methods. 
There is no cross validation performed due to high compution time. 


## apply model to test set
apply model to the test set to make predicitons
```{r}
test_predict <- predict(decisiontree, newdata= df_test)
summary(test_predict)
```




